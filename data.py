"""
data_pipeline.py
================
Module t·ª± ƒë·ªông h√≥a quy tr√¨nh:
1. Crawl d·ªØ li·ªáu BƒêS t·ª´ Ch·ª£ T·ªët
2. Upload l√™n MongoDB
3. Load d·ªØ li·ªáu t·ª´ MongoDB v·ªÅ DataFrame

Author: Your Name
Date: 2024
"""

import requests
import pandas as pd
import time
from tqdm import tqdm
from pymongo import MongoClient
import certifi

# ============================================================================
# PH·∫¶N 1: CRAWL D·ªÆ LI·ªÜU T·ª™ CH·ª¢ T·ªêT
# ============================================================================

def crawl_chotot_data(
    start_page=0,
    max_pages=None,
    save_every=500,
    sleep_time=1,
    region_id=None,
    save_csv=True,
    csv_filename=None
):
    """
    Crawl d·ªØ li·ªáu BƒêS t·ª´ Ch·ª£ T·ªët - ƒë·∫ßy ƒë·ªß th√¥ng tin, t·ª± ƒë·ªông mapping h∆∞·ªõng & ph√°p l√Ω.

    Parameters
    ----------
    start_page : int, optional
        Trang b·∫Øt ƒë·∫ßu crawl (m·∫∑c ƒë·ªãnh l√† 0).
    max_pages : int ho·∫∑c None, optional
        S·ªë l∆∞·ª£ng trang t·ªëi ƒëa mu·ªën crawl (m·∫∑c ƒë·ªãnh l√† None - kh√¥ng gi·ªõi h·∫°n).
    save_every : int, optional
        T·ª± ƒë·ªông L∆ØU file CSV t·∫°m sau m·ªói N tin (m·∫∑c ƒë·ªãnh l√† 500).
    sleep_time : float, optional
        Th·ªùi gian (gi√¢y) ngh·ªâ gi·ªØa m·ªói l·∫ßn g·ªçi API.
    region_id : str ho·∫∑c None, optional
        M√£ v√πng (region) mu·ªën l·ªçc.
        - "0" ho·∫∑c None: To√†n qu·ªëc (m·∫∑c ƒë·ªãnh).
        - "13000": Ch·ªâ TP. H·ªì Ch√≠ Minh.
        - "12000": Ch·ªâ H√† N·ªôi.
    save_csv : bool, optional
        C√≥ l∆∞u file CSV cu·ªëi c√πng kh√¥ng (m·∫∑c ƒë·ªãnh True).
    csv_filename : str, optional
        T√™n file CSV t√πy ch·ªânh (n·∫øu kh√¥ng c√≥ s·∫Ω t·ª± ƒë·ªông ƒë·∫∑t t√™n).

    Returns
    -------
    pandas.DataFrame
        DataFrame ch·ª©a to√†n b·ªô d·ªØ li·ªáu ƒë√£ thu th·∫≠p v√† l√†m s·∫°ch.
    """

    # =========================
    # C·∫§U H√åNH
    # =========================
    BASE_URL = "https://gateway.chotot.com/v1/public/ad-listing"

    KEYS_TO_EXTRACT = [
        "list_id", "status", "price", "price_string", "price_million_per_m2",
        "size", "width", "length", "rooms", "direction", "property_legal_document",
        "region_name", "area_name", "ward_name", "category_name", "is_main_street",
        "number_of_images"
    ]

    all_data = []
    all_ids = set()
    file_counter = 1
    page = start_page
    consecutive_errors = 0
    MAX_CONSECUTIVE_ERRORS = 5

    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    })

    # =========================
    # H√ÄM PH·ª§ TR·ª¢
    # =========================
    def safe_get(ad, key, default=''):
        value = ad.get(key, default)
        return value if value is not None else default

    def map_values(df):
        direction_map = {
            1: "ƒê√¥ng", 2: "T√¢y", 3: "Nam", 4: "B·∫Øc",
            5: "ƒê√¥ng-B·∫Øc", 6: "T√¢y-B·∫Øc", 7: "ƒê√¥ng-Nam", 8: "T√¢y-Nam"
        }
        legal_doc_map = {
            1: "S·ªï h·ªìng / S·ªï ƒë·ªè ƒë·∫ßy ƒë·ªß", 2: "Gi·∫•y tay / Ch∆∞a c√≥ s·ªï",
            3: "ƒêang ch·ªù s·ªï", 4: "H·ª£p ƒë·ªìng mua b√°n", 5: "Kh√°c"
        }
        df["direction_text"] = df["direction"].map(direction_map)
        df["legal_doc_text"] = df["property_legal_document"].map(legal_doc_map)
        return df

    def save_snapshot(data_list, file_num, current_page):
        if not data_list:
            return
        df_temp = pd.DataFrame(data_list)
        filename = f"chotot_part{file_num}_{len(data_list)}tin_p{current_page}.csv"
        df_temp.to_csv(filename, index=False, encoding='utf-8-sig')
        tqdm.write(f"\nüíæ [Snapshot {file_num}] ƒê√£ l∆∞u: {filename} ({len(data_list)} tin)")

    # =========================
    # V√íNG L·∫∂P CRAWL
    # =========================
    region_text = "TO√ÄN QU·ªêC üáªüá≥" if region_id in [None, "0"] else f"Region {region_id}"
    print(f"üöÄ B·∫Øt ƒë·∫ßu crawl t·ª´ trang {start_page} | V√πng: {region_text}")
    print(f"üíæ T·ª± ƒë·ªông l∆∞u snapshot m·ªói {save_every} tin")
    print(f"üõë D·ª´ng n·∫øu g·∫∑p {MAX_CONSECUTIVE_ERRORS} l·ªói li√™n ti·∫øp.\n")

    pbar = tqdm(total=max_pages, desc="Crawling", unit="page")

    while True:
        if max_pages is not None and (page - start_page) >= max_pages:
            tqdm.write(f"\n‚èπÔ∏è ƒê√£ ƒë·∫°t gi·ªõi h·∫°n {max_pages} trang. D·ª´ng crawl.")
            break

        offset = page * 25
        params = {"cg": 1000, "o": offset, "st": "s,k", "limit": 25}
        if region_id and region_id != "0":
            params["region_v2"] = region_id

        try:
            r = session.get(BASE_URL, params=params, timeout=15)
            r.raise_for_status()
            data = r.json()
            consecutive_errors = 0

        except requests.exceptions.HTTPError as e:
            tqdm.write(f"\n‚ö†Ô∏è L·ªói HTTP trang {page}: {e}")
            if r.status_code == 404:
                tqdm.write("L·ªói 404, ƒë√£ h·∫øt trang. D·ª´ng...")
                break

            consecutive_errors += 1
            tqdm.write(f"L·ªói li√™n ti·∫øp: {consecutive_errors}/{MAX_CONSECUTIVE_ERRORS}")
            if consecutive_errors >= MAX_CONSECUTIVE_ERRORS:
                tqdm.write("üõë ƒê·∫°t t·ªëi ƒëa l·ªói li√™n ti·∫øp. D·ª´ng crawl.")
                break

            time.sleep(5)
            continue

        except Exception as e:
            tqdm.write(f"\n‚ö†Ô∏è L·ªói k·∫øt n·ªëi trang {page}: {e}")
            consecutive_errors += 1
            tqdm.write(f"L·ªói li√™n ti·∫øp: {consecutive_errors}/{MAX_CONSECUTIVE_ERRORS}")
            if consecutive_errors >= MAX_CONSECUTIVE_ERRORS:
                tqdm.write("üõë ƒê·∫°t t·ªëi ƒëa l·ªói li√™n ti·∫øp. D·ª´ng crawl.")
                break

            time.sleep(5)
            continue

        ads = data.get('ads', [])

        if not ads:
            tqdm.write(f"\n‚úÖ API tr·∫£ v·ªÅ r·ªóng t·∫°i trang {page}. D·ª´ng crawl.")
            break

        new_records_on_page = 0
        for ad in ads:
            list_id = safe_get(ad, 'list_id')
            if not list_id or list_id in all_ids:
                continue

            all_ids.add(list_id)
            new_records_on_page += 1
            record = {key: safe_get(ad, key) for key in KEYS_TO_EXTRACT}
            all_data.append(record)

        if new_records_on_page == 0:
            tqdm.write(f"\nüõë Trang {page} kh√¥ng c√≥ tin m·ªõi. D·ª´ng crawl.")
            break

        page += 1
        pbar.update(1)
        pbar.set_postfix({"T·ªïng tin": len(all_data)})

        if len(all_data) >= file_counter * save_every:
            save_snapshot(all_data, file_counter, page)
            file_counter += 1

        time.sleep(sleep_time)

    pbar.close()

    # =========================
    # HO√ÄN TH√ÄNH
    # =========================
    if not all_data:
        print("\n‚ùå Crawl ho√†n t·∫•t nh∆∞ng kh√¥ng c√≥ d·ªØ li·ªáu.")
        return pd.DataFrame()

    print(f"\nüéâ Crawl ho√†n t·∫•t! T·ªïng c·ªông {len(all_data)} tin.")

    df_final = pd.DataFrame(all_data)
    df_final = map_values(df_final)

    column_order = [
        "list_id", "status", "price", "price_string", "price_million_per_m2",
        "size", "width", "length", "rooms", "direction", "direction_text",
        "property_legal_document", "legal_doc_text", "region_name", "area_name",
        "ward_name", "category_name", "is_main_street", "number_of_images",
    ]
    df_final = df_final.reindex(columns=column_order)

    # L∆∞u file CSV
    if save_csv:
        if csv_filename is None:
            region_suffix = "TOANQUOC" if region_id in [None, "0"] else f"region{region_id}"
            csv_filename = f"chotot_FINAL_{region_suffix}_{len(df_final)}tin_p{start_page}-p{page-1}.csv"

        print(f"üíæ ƒêang l∆∞u file: {csv_filename}...")
        df_final.to_csv(csv_filename, index=False, encoding='utf-8-sig')
        print(f"‚úÖ ƒê√£ l∆∞u file th√†nh c√¥ng!")

    return df_final


# ============================================================================
# PH·∫¶N 2: UPLOAD L√äN MONGODB
# ============================================================================

def upload_to_mongodb(
    df,
    connection_string,
    db_name,
    collection_name,
    drop=0
):
    """
    ƒê·∫©y DataFrame l√™n MongoDB collection.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame c·∫ßn upload.
    connection_string : str
        Chu·ªói k·∫øt n·ªëi MongoDB.
    db_name : str
        T√™n database.
    collection_name : str
        T√™n collection.
    drop : int, optional
        - drop = 0: Gi·ªØ d·ªØ li·ªáu c≈©, ch√®n th√™m (m·∫∑c ƒë·ªãnh).
        - drop = 1: X√≥a t·∫•t c·∫£ d·ªØ li·ªáu c≈© tr∆∞·ªõc khi ch√®n.

    Returns
    -------
    bool
        True n·∫øu th√†nh c√¥ng, False n·∫øu c√≥ l·ªói.
    """

    if df.empty:
        print("‚ö†Ô∏è DataFrame r·ªóng, kh√¥ng c√≥ g√¨ ƒë·ªÉ upload.")
        return False

    client = None

    try:
        # K·∫øt n·ªëi MongoDB
        ca = certifi.where()
        client = MongoClient(connection_string, tls=True, tlsCAFile=ca)

        db = client[db_name]
        collection = db[collection_name]
        print(f"‚úÖ ƒê√£ k·∫øt n·ªëi t·ªõi DB: '{db_name}', Collection: '{collection_name}'")

        # X√≥a d·ªØ li·ªáu c≈© n·∫øu drop=1
        if drop == 1:
            print("üóëÔ∏è  ƒêang x√≥a d·ªØ li·ªáu c≈© (drop=1)...")
            result_delete = collection.delete_many({})
            print(f"‚úÖ ƒê√£ x√≥a {result_delete.deleted_count} document c≈©.")
        else:
            print("‚ûï Ch√®n n·ªëi ti·∫øp (drop=0)...")

        # Chuy·ªÉn DataFrame sang list dict
        data_to_insert = df.to_dict("records")

        # Upload
        result_insert = collection.insert_many(data_to_insert)
        print(f"‚úÖ ƒê√£ upload th√†nh c√¥ng {len(result_insert.inserted_ids)} document!\n")

        return True

    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
        if "SSL" in str(e) or "timeout" in str(e):
            print("‚ö†Ô∏è G·ª¢I √ù: Ki·ªÉm tra IP Access List tr√™n MongoDB Atlas!")
        return False

    finally:
        if client:
            client.close()


# ============================================================================
# PH·∫¶N 3: LOAD D·ªÆ LI·ªÜU T·ª™ MONGODB
# ============================================================================

def load_from_mongodb(
    connection_string,
    db_name,
    collection_name,
    query_filter={},
    remove_id=True
):
    """
    K·∫øt n·ªëi MongoDB v√† t·∫£i d·ªØ li·ªáu v·ªÅ DataFrame.

    Parameters
    ----------
    connection_string : str
        Chu·ªói k·∫øt n·ªëi MongoDB.
    db_name : str
        T√™n database.
    collection_name : str
        T√™n collection.
    query_filter : dict, optional
        B·ªô l·ªçc MongoDB (m·∫∑c ƒë·ªãnh {} = l·∫•y t·∫•t c·∫£).
    remove_id : bool, optional
        N·∫øu True, t·ª± ƒë·ªông lo·∫°i b·ªè c·ªôt "_id" (m·∫∑c ƒë·ªãnh True).

    Returns
    -------
    pandas.DataFrame
        DataFrame ch·ª©a d·ªØ li·ªáu. Tr·∫£ v·ªÅ DataFrame r·ªóng n·∫øu c√≥ l·ªói.
    """

    df_from_mongo = pd.DataFrame()
    client = None

    try:
        # K·∫øt n·ªëi MongoDB
        ca = certifi.where()
        client = MongoClient(connection_string, tls=True, tlsCAFile=ca)

        db = client[db_name]
        collection = db[collection_name]

        print(f"üì• ƒêang t·∫£i d·ªØ li·ªáu t·ª´ '{db_name}.{collection_name}'...")

        # Projection ƒë·ªÉ lo·∫°i b·ªè _id
        projection = {"_id": 0} if remove_id else None

        # Truy v·∫•n
        cursor = collection.find(query_filter, projection)
        data_list = list(cursor)

        if data_list:
            df_from_mongo = pd.DataFrame(data_list)
            print(f"‚úÖ T·∫£i th√†nh c√¥ng {len(df_from_mongo)} document.\n")
        else:
            print("‚ö†Ô∏è Collection r·ªóng ho·∫∑c kh√¥ng c√≥ d·ªØ li·ªáu.\n")

    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
        if "SSL" in str(e) or "timeout" in str(e):
            print("‚ö†Ô∏è G·ª¢I √ù: Ki·ªÉm tra IP Access List tr√™n MongoDB Atlas!")

    finally:
        if client:
            client.close()

    return df_from_mongo


# ============================================================================
# PH·∫¶N 4: PIPELINE T·ª∞ ƒê·ªòNG (ALL-IN-ONE)
# ============================================================================

def run_full_pipeline(
    # Crawl params
    max_pages=100,
    region_id=None,
    sleep_time=1,
    # MongoDB params
    connection_string=None,
    db_name=None,
    collection_name=None,
    drop_before_upload=0,
    # Options
    skip_crawl=False,
    skip_upload=False,
    skip_load=False
):
    """
    Ch·∫°y to√†n b·ªô pipeline: Crawl ‚Üí Upload ‚Üí Load

    Parameters
    ----------
    max_pages : int
        S·ªë trang t·ªëi ƒëa ƒë·ªÉ crawl.
    region_id : str, optional
        M√£ v√πng (None = to√†n qu·ªëc).
    sleep_time : float
        Th·ªùi gian ch·ªù gi·ªØa c√°c request.
    connection_string : str
        Connection string MongoDB.
    db_name : str
        T√™n database MongoDB.
    collection_name : str
        T√™n collection MongoDB.
    drop_before_upload : int
        0 = append, 1 = drop tr∆∞·ªõc khi upload.
    skip_crawl : bool
        N·∫øu True, b·ªè qua b∆∞·ªõc crawl (d√πng data c√≥ s·∫µn).
    skip_upload : bool
        N·∫øu True, b·ªè qua b∆∞·ªõc upload.
    skip_load : bool
        N·∫øu True, b·ªè qua b∆∞·ªõc load.

    Returns
    -------
    pandas.DataFrame
        DataFrame cu·ªëi c√πng (t·ª´ MongoDB ho·∫∑c t·ª´ crawl).
    """

    print("="*80)
    print("üöÄ B·∫ÆT ƒê·∫¶U FULL PIPELINE")
    print("="*80 + "\n")

    df_final = None

    # ========================================
    # B∆Ø·ªöC 1: CRAWL D·ªÆ LI·ªÜU
    # ========================================
    if not skip_crawl:
        print("\n" + "="*80)
        print("üì° B∆Ø·ªöC 1: CRAWL D·ªÆ LI·ªÜU T·ª™ CH·ª¢ T·ªêT")
        print("="*80 + "\n")

        df_crawled = crawl_chotot_data(
            max_pages=max_pages,
            region_id=region_id,
            sleep_time=sleep_time,
            save_csv=True
        )

        if df_crawled.empty:
            print("‚ùå Crawl kh√¥ng c√≥ d·ªØ li·ªáu. D·ª´ng pipeline.")
            return pd.DataFrame()

        df_final = df_crawled

    else:
        print("\n‚è≠Ô∏è  B·ªè qua b∆∞·ªõc crawl (skip_crawl=True)")

    # ========================================
    # B∆Ø·ªöC 2: UPLOAD L√äN MONGODB
    # ========================================
    if not skip_upload and df_final is not None:
        if connection_string is None or db_name is None or collection_name is None:
            print("\n‚ö†Ô∏è Thi·∫øu th√¥ng tin MongoDB. B·ªè qua upload.")
        else:
            print("\n" + "="*80)
            print("‚òÅÔ∏è  B∆Ø·ªöC 2: UPLOAD L√äN MONGODB")
            print("="*80 + "\n")

            success = upload_to_mongodb(
                df_final,
                connection_string,
                db_name,
                collection_name,
                drop=drop_before_upload
            )

            if not success:
                print("‚ö†Ô∏è Upload th·∫•t b·∫°i nh∆∞ng ti·∫øp t·ª•c pipeline.")
    else:
        print("\n‚è≠Ô∏è  B·ªè qua b∆∞·ªõc upload")

    # ========================================
    # B∆Ø·ªöC 3: LOAD T·ª™ MONGODB
    # ========================================
    if not skip_load:
        if connection_string is None or db_name is None or collection_name is None:
            print("\n‚ö†Ô∏è Thi·∫øu th√¥ng tin MongoDB. B·ªè qua load.")
        else:
            print("\n" + "="*80)
            print("üì• B∆Ø·ªöC 3: LOAD D·ªÆ LI·ªÜU T·ª™ MONGODB")
            print("="*80 + "\n")

            df_loaded = load_from_mongodb(
                connection_string,
                db_name,
                collection_name
            )

            if not df_loaded.empty:
                df_final = df_loaded
            else:
                print("‚ö†Ô∏è Load kh√¥ng c√≥ d·ªØ li·ªáu.")
    else:
        print("\n‚è≠Ô∏è  B·ªè qua b∆∞·ªõc load")

    # ========================================
    # HO√ÄN TH√ÄNH
    # ========================================
    print("\n" + "="*80)
    print("üéâ HO√ÄN T·∫§T FULL PIPELINE")
    print("="*80)

    if df_final is not None and not df_final.empty:
        print(f"\nüìä K·∫øt qu·∫£ cu·ªëi c√πng: {df_final.shape[0]} d√≤ng, {df_final.shape[1]} c·ªôt")
        print("\nüìå 5 d√≤ng ƒë·∫ßu:")
        print(df_final.head())
    else:
        print("\n‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu cu·ªëi c√πng.")

    return df_final


# # ========================================
    # # C√ÅCH 1: CH·∫†Y FULL PIPELINE
    # # ========================================
    # print("\nüî• CH·∫†Y FULL PIPELINE (Crawl ‚Üí Upload ‚Üí Load)\n")

    # df_result = run_full_pipeline(
    #     max_pages=10,  # Crawl 10 trang ƒë·ªÉ test
    #     region_id=None,  # None = to√†n qu·ªëc
    #     sleep_time=1,
    #     connection_string=MONGO_CONNECTION,
    #     db_name=DB_NAME,
    #     collection_name=COLLECTION_NAME,
    #     drop_before_upload=0  # 0 = append, 1 = drop tr∆∞·ªõc khi upload
    # )

    # ========================================
    # C√ÅCH 2: CH·∫†Y T·ª™NG B∆Ø·ªöC RI√äNG L·∫∫
    # ========================================

    # # B∆Ø·ªöC 1: Ch·ªâ crawl
    # df_crawled = crawl_chotot_data(max_pages=10, region_id="13000")

    # # B∆Ø·ªöC 2: Ch·ªâ upload
    # upload_to_mongodb(df_crawled, MONGO_CONNECTION, DB_NAME, COLLECTION_NAME, drop=1)

    # # B∆Ø·ªöC 3: Ch·ªâ load
    # df_loaded = load_from_mongodb(MONGO_CONNECTION, DB_NAME, COLLECTION_NAME)

    # ========================================
    # C√ÅCH 3: CH·ªà LOAD D·ªÆ LI·ªÜU C√ì S·∫¥N
    # ========================================

    # df_from_db = run_full_pipeline(
    #     connection_string=MONGO_CONNECTION,
    #     db_name=DB_NAME,
    #     collection_name=COLLECTION_NAME,
    #     skip_crawl=True,  # B·ªè qua crawl
    #     skip_upload=True  # B·ªè qua upload
    # )